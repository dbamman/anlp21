{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5792cb96",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore the basics of token representations in BERT and use it to find token nearest neighbors.  Before using, be sure to install the following libraries:\n",
    "\n",
    "```sh\n",
    "conda install -c conda-forge ipywidgets\n",
    "conda install -c huggingface transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a12638",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f34541",
   "metadata": {},
   "source": [
    "BERT uses WordPiece tokenization, which breaks down words that don't appear within its 30K-word vocabulary into small pieces.  The word \"vaccinated\", for instanced, is tokenized as [\"va\", \"##cci\", \"##nated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"New data shows 26 states have fully vaccinated more than half their residents.\", return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"BERT is supercalifragilisticexpialidocious\", return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058b2be",
   "metadata": {},
   "source": [
    "As mentioned in class, notice how every sentence input to BERT is wrapped in two tags: a start [CLS] tag and an ending [SEP] tag.  BERT will generate representations of each WordPiece token, including these special [CLS] and [SEP] tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9c2fa",
   "metadata": {},
   "source": [
    "To generate representations for the input tokens, simply pass the input through the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"This jam is delicious\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3f309",
   "metadata": {},
   "source": [
    "Representations for each of BERT layers (12 in this model) are accessible here, but let's explore just the outputs from the final layer.  This BERT model has 768-dimensional representations, so this 6-token input ([CLS, this, jam, is, delicious, [SEP]) has an output that is is a 1 x 6 tokens x 768 dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe68cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888e87d",
   "metadata": {},
   "source": [
    "We'll cover using BERT for supervised problems in later classes, but what can we do with just these representations?  While we used word2vec-style static embeddings to find nearest neighbors for word *types*, we can do the same here for word *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"I ate some jam with toast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sents=[\"She got me out of a real jam\", \"This jam is made of strawberries\", \"I sat in a traffic jam for 2 hours\", \"The Grateful Dead used to jam for like two days straight.\", \"My grandma makes the best jam.\", \"I had to jam on the brakes to avoid hitting him.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be898d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_for_token(string, term):\n",
    "    \n",
    "    # tokenize\n",
    "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "    \n",
    "    # convert input ids to words\n",
    "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    # find the first location of the query term among those tokens (so we know which BERT rep to use)\n",
    "    term_idx=tokens.index(term)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # return the BERT rep for that token index\n",
    "    # The output is a pytorch tensor object, but let's convert it to a numpy object to work with numpy functions\n",
    "    \n",
    "    return outputs.last_hidden_state[0][term_idx].detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719345c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rep=get_bert_for_token(query, \"jam\")\n",
    "print(query_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0e1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals=[]\n",
    "for sent in comp_sents:\n",
    "    comp_rep=get_bert_for_token(sent, \"jam\")\n",
    "    cos_sim=cosine_similarity(query_rep, comp_rep)\n",
    "    vals.append((cos_sim, query, sent))\n",
    "\n",
    "for c, q, s in reversed(sorted(vals)):\n",
    "    print(\"%.3f\\t%s\\t%s\" % (c, q, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7167cc",
   "metadata": {},
   "source": [
    "**Q**: Brainstorm the variety of things you can do with token-level word representations like this and we'll discuss them at the end of class.  As one example, adapt the code above to find *any* word that is most similar to *jam* in \"I ate some jam with toast\" in the following sentences.  Are you able to find token-level synonyms this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sents=[\"My grandma makes the best jelly.\", \"Jelly is made of strawberries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42395734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
