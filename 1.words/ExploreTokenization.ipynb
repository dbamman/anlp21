{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines several methods for tokenizing text into words (and sentences), including:\n",
    "\n",
    "* whitespace\n",
    "* nltk (Penn Treebank tokenizer)\n",
    "* nltk (Twitter-aware)\n",
    "* spaCy\n",
    "* custom regular expressions\n",
    "\n",
    "highlighting differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dbamman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you haven't downloaded the sentence segmentation model before, do so here\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy lemmatization needs tagger but disable the rest\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger,ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_from_json(filename):\n",
    "    tweets=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "#         data=json.load(file)\n",
    "#         for tweet in data:\n",
    "            tweet=json.loads(line)\n",
    "            tweets.append(tweet[\"full_text\"])\n",
    "    return tweets        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "potus_tweets.json contains 1384 tweets from the Twitter @POTUS account, downloaded on 8/30/21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"../data/potus_tweets.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=read_tweets_from_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokens=[]\n",
    "for tweet in tweets:\n",
    "    whitespace_tokens.append(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_tokens.append(nltk.word_tokenize(tweet, language=\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_casual_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_casual_tokens.append(nltk.casual_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens=[]\n",
    "for tweet in tweets:\n",
    "    spacy_tokens.append([token.text for token in nlp(tweet)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter version of http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "\n",
    "# The order here is important (match from first to last)\n",
    "\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize(text):\n",
    "    return my_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensible_tokens=[]\n",
    "for tweet in tweets:\n",
    "    extensible_tokens.append(my_extensible_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a function to print out the first 5 tokenized tweets in each of the five tokenizers above. Examine those tweets; how would you characterize the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK      :\tThe past 17 days have seen our troops execute the largest airlift in US history . They have done it with unmatched courage , professionalism , and resolve . Now , our 20-year military presence in Afghanistan has ended . My full statement : https : //t.co/kfLkzQtEzp\n",
      "CASUAL    :\tThe past 17 days have seen our troops execute the largest airlift in US history . They have done it with unmatched courage , professionalism , and resolve . Now , our 20 - year military presence in Afghanistan has ended . My full statement : https://t.co/kfLkzQtEzp\n",
      "SPACY     :\tThe past 17 days have seen our troops execute the largest airlift in US history . They have done it with unmatched courage , professionalism , and resolve . Now , our 20 - year military presence in Afghanistan has ended . \n",
      "\n",
      " My full statement : https://t.co/kfLkzQtEzp\n",
      "WHITESPACE:\tThe past 17 days have seen our troops execute the largest airlift in US history. They have done it with unmatched courage, professionalism, and resolve. Now, our 20-year military presence in Afghanistan has ended. My full statement: https://t.co/kfLkzQtEzp\n",
      "EXTENSIBLE:\tThe past 17 days have seen our troops execute the largest airlift in US history . They have done it with unmatched courage , professionalism , and resolve . Now , our 20 - year military presence in Afghanistan has ended . My full statement : https : / / t . co / kfLkzQtEzp\n",
      "\n",
      "NLTK      :\tTune in as I deliver remarks on the response to Hurricane Ida . https : //t.co/6WDLmsVj4o\n",
      "CASUAL    :\tTune in as I deliver remarks on the response to Hurricane Ida . https://t.co/6WDLmsVj4o\n",
      "SPACY     :\tTune in as I deliver remarks on the response to Hurricane Ida . https://t.co/6WDLmsVj4o\n",
      "WHITESPACE:\tTune in as I deliver remarks on the response to Hurricane Ida. https://t.co/6WDLmsVj4o\n",
      "EXTENSIBLE:\tTune in as I deliver remarks on the response to Hurricane Ida . https : / / t . co / 6WDLmsVj4o\n",
      "\n",
      "NLTK      :\tRT @ WhiteHouse : Since August 14 , the U.S. has evacuated and facilitated the evacuation of approximately 116,700 people . Since the end of Ju…\n",
      "CASUAL    :\tRT @WhiteHouse : Since August 14 , the U . S . has evacuated and facilitated the evacuation of approximately 116,700 people . Since the end of Ju …\n",
      "SPACY     :\tRT @WhiteHouse : Since August 14 , the U.S. has evacuated and facilitated the evacuation of approximately 116,700 people . Since the end of Ju …\n",
      "WHITESPACE:\tRT @WhiteHouse: Since August 14, the U.S. has evacuated and facilitated the evacuation of approximately 116,700 people. Since the end of Ju…\n",
      "EXTENSIBLE:\tRT @WhiteHouse : Since August 14 , the U . S . has evacuated and facilitated the evacuation of approximately 116 , 700 people . Since the end of Ju …\n",
      "\n",
      "NLTK      :\tRT @ WhiteHouse : From 3 AM ET on 8/29 to 3AM ET on 8/30 , a total of approximately 1,200 people were evacuated from Kabul . 26 US military fli…\n",
      "CASUAL    :\tRT @WhiteHouse : From 3 AM ET on 8/ 29 to 3AM ET on 8/ 30 , a total of approximately 1,200 people were evacuated from Kabul . 26 US military fli …\n",
      "SPACY     :\tRT @WhiteHouse : From 3 AM ET on 8/29 to 3AM ET on 8/30 , a total of approximately 1,200 people were evacuated from Kabul . 26 US military fli …\n",
      "WHITESPACE:\tRT @WhiteHouse: From 3 AM ET on 8/29 to 3AM ET on 8/30, a total of approximately 1,200 people were evacuated from Kabul. 26 US military fli…\n",
      "EXTENSIBLE:\tRT @WhiteHouse : From 3 AM ET on 8 / 29 to 3AM ET on 8 / 30 , a total of approximately 1 , 200 people were evacuated from Kabul . 26 US military fli …\n",
      "\n",
      "NLTK      :\tThanks to the hard work of @ FEMA , we ’ ve pre-positioned resources , equipment , and response teams to respond to Hurricane Ida . That includes more than 2,400 FEMA employees , millions of meals and liters of water , generators , search and rescue teams , and over 100 ambulances . https : //t.co/oPNBSj4hVS\n",
      "CASUAL    :\tThanks to the hard work of @FEMA , we ’ ve pre-positioned resources , equipment , and response teams to respond to Hurricane Ida . That includes more than 2,400 FEMA employees , millions of meals and liters of water , generators , search and rescue teams , and over 100 ambulances . https://t.co/oPNBSj4hVS\n",
      "SPACY     :\tThanks to the hard work of @FEMA , we ’ve pre - positioned resources , equipment , and response teams to respond to Hurricane Ida . That includes more than 2,400 FEMA employees , millions of meals and liters of water , generators , search and rescue teams , and over 100 ambulances . https://t.co/oPNBSj4hVS\n",
      "WHITESPACE:\tThanks to the hard work of @FEMA, we’ve pre-positioned resources, equipment, and response teams to respond to Hurricane Ida. That includes more than 2,400 FEMA employees, millions of meals and liters of water, generators, search and rescue teams, and over 100 ambulances. https://t.co/oPNBSj4hVS\n",
      "EXTENSIBLE:\tThanks to the hard work of @FEMA , we’ve pre-positioned resources , equipment , and response teams to respond to Hurricane Ida . That includes more than 2 , 400 FEMA employees , millions of meals and liters of water , generators , search and rescue teams , and over 100 ambulances . https : / / t . co / oPNBSj 4hVS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (one, two, three, four, five) in enumerate(zip(nltk_tokens, nltk_casual_tokens, spacy_tokens, whitespace_tokens, extensible_tokens)):\n",
    "    if idx >= 5:\n",
    "        break\n",
    "    print(\"NLTK      :\\t%s\" % ' '.join(one))\n",
    "    print(\"CASUAL    :\\t%s\" % ' '.join(two))\n",
    "    print(\"SPACY     :\\t%s\" % ' '.join(three))\n",
    "    print(\"WHITESPACE:\\t%s\" % ' '.join(four))\n",
    "    print(\"EXTENSIBLE:\\t%s\" % ' '.join(five))\n",
    "\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a function `compare(tokenization_one, tokenization_two)` that compares two tokenizations of the same text and finds the 20 most frequent tokens that don't appear in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(one_tokens, two_tokens):\n",
    "    \n",
    "    one_counts=Counter()\n",
    "    two_counts=Counter()\n",
    "\n",
    "    for sentence in one_tokens:\n",
    "        for token in sentence:\n",
    "            one_counts[token]+=1\n",
    "        \n",
    "    for sentence in two_tokens:\n",
    "        for token in sentence:\n",
    "            two_counts[token]+=1\n",
    "        \n",
    "    missing_from_one=Counter()\n",
    "    missing_from_two=Counter()\n",
    "    \n",
    "    for word_type in one_counts:\n",
    "        if word_type not in two_counts:\n",
    "            missing_from_two[word_type]=one_counts[word_type]\n",
    "        \n",
    "    for word_type in two_counts:\n",
    "        if word_type not in one_counts:\n",
    "            missing_from_one[word_type]=two_counts[word_type]\n",
    "\n",
    "    print (\"Token counts -- one: %s, two: %s\" % (len(one_tokens), len(two_tokens)))\n",
    "    print (\"\\nNot in one:\")\n",
    "    print ('\\n'.join(\"%s\\t%d\" % (k,v) for (k,v) in missing_from_one.most_common(20)))\n",
    "    print (\"\\nNot in two:\")\n",
    "    print ('\\n'.join(\"%s\\t%d\" % (k,v) for (k,v) in missing_from_two.most_common(20)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counts -- one: 1384, two: 1384\n",
      "\n",
      "Not in one:\n",
      "https\t767\n",
      "@\t142\n",
      "COVID-19\t121\n",
      "WhiteHouse\t40\n",
      "'s\t38\n",
      "U.S.\t30\n",
      "#\t17\n",
      "//t.co/4MYpWqXVVo\t16\n",
      "'re\t13\n",
      "amp\t12\n",
      ";\t12\n",
      "LGBTQ+\t11\n",
      "//t.co/gRX1fGFEzj\t9\n",
      "Dr.\t9\n",
      "VP\t8\n",
      "n't\t8\n",
      "JustinTrudeau\t6\n",
      "FLOTUS\t6\n",
      "St.\t5\n",
      "TeamUSA\t4\n",
      "\n",
      "Not in two:\n",
      "…\t47\n",
      "U\t41\n",
      "S\t41\n",
      "@WhiteHouse\t40\n",
      "https://t.co/4MYpWqXVVo\t16\n",
      "8/\t12\n",
      "+\t12\n",
      "That's\t11\n",
      "LGBTQ\t11\n",
      "cannot\t10\n",
      "https://t.co/gRX1fGFEzj\t9\n",
      "Dr\t9\n",
      "@VP\t8\n",
      "H\t8\n",
      "It's\t8\n",
      "K\t7\n",
      "R\t6\n",
      "@JustinTrudeau\t6\n",
      "@FLOTUS\t6\n",
      "We're\t5\n"
     ]
    }
   ],
   "source": [
    "compare(nltk_casual_tokens, nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Use one of the NLTK tokenizers; write code to determine how many sentences are in this dataset, and what the average number of words per sentence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents: 3567, Tokens/sent: 14.8\n"
     ]
    }
   ],
   "source": [
    "count=0.\n",
    "num_sents=0\n",
    "for tweet in tweets:\n",
    "    for sent in nltk.sent_tokenize(tweet):\n",
    "        count+=len(nltk.word_tokenize(sent))\n",
    "        num_sents+=1\n",
    "print(\"Sents: %s, Tokens/sent: %.1f\" % (num_sents, (count/num_sents)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4 (check-plus): modify the extensible tokenizer above to keep urls together (e.g., www.google.com or http://www.google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep urls together\n",
    "r\"(?:https?:\\S+)\",\n",
    "r\"(?:www\\.\\S+)\",\n",
    "  \n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_url_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize_with_urls(text):\n",
    "    return my_url_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "course\n",
      "website\n",
      "is\n",
      "http://people.ischool.berkeley.edu/~dbamman/info256.html\n"
     ]
    }
   ],
   "source": [
    "print ('\\n'.join(my_extensible_tokenize_with_urls(\"The course website is http://people.ischool.berkeley.edu/~dbamman/info256.html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
