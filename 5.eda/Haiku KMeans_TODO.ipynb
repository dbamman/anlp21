{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8780af9",
   "metadata": {},
   "source": [
    "In \"[Literary Pattern Recognition](https://www.journals.uchicago.edu/doi/full/10.1086/684353)\", Long and So train a classifier to differentiate haiku poems from non-haiku poems, and find that many features help do so.  In class, we've discussed the importance of representation--how you *describe* a text computationally influences the kinds of things you are able to do with it.  While Long and So explore description in the context of classification, in this homework, you'll see how well you can design features that can differentiate these two classes *without* any supervision. Are you able to featurize a collection of poems such that two clusters (haiku/non-haiku) emerge when using KMeans clustering, with the text representation as your only degree of freedom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ae833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, re\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f517024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(path, metadata, filepath_col):\n",
    "    data=[]\n",
    "    with open(metadata, encoding=\"utf-8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)\n",
    "        for cols in csv_reader:\n",
    "            poem_path=os.path.join(path, cols[filepath_col])\n",
    "            if os.path.exists(poem_path):\n",
    "                with open(poem_path, encoding=\"utf-8\") as poem_file:\n",
    "                    poem=poem_file.read()\n",
    "                    data.append(poem)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81026d2",
   "metadata": {},
   "source": [
    "Here we'll use data originally released on Github to support \"Literary Pattern Recognition\": [https://github.com/hoytlong/PatternRecognition](https://github.com/hoytlong/PatternRecognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku=read_texts(\"../data/haiku/long_so_haiku\", \"../data/haiku/Haikus.csv\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336544fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "others=read_texts(\"../data/haiku/long_so_others\", \"../data/haiku/OthersData.csv\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change anything within this code block\n",
    "\n",
    "def run_all(haiku, others, feature_function):\n",
    "    \n",
    "    X, Y, featurize_vocab=feature_function(haiku, others)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "    nmi=metrics.normalized_mutual_info_score(Y, kmeans.labels_)\n",
    "    print(\"%.3f NMI\" % nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c16b1",
   "metadata": {},
   "source": [
    "As one example, let's take a simple featurization and represent each poem by a binary indicator of the dictionary word types it contains.  \"To be or not to be\", for example, would be represented as {\"to\": 1, \"be\": 1, \"or\": 1, \"not\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a list of haiku poems and non-haiku poems, and returns:\n",
    "\n",
    "# X (sparse matrix, with poems as rows and features as columns)\n",
    "# Y (list of poem labels, with 1=haiku and 0=non-haiku)\n",
    "# feature_vocab (dict mapping feature name to feature ID)\n",
    "\n",
    "def unigram_featurize_all(haiku, others):\n",
    "\n",
    "    def unigram_featurize(poem, feature_vocab):\n",
    "        \n",
    "        # featurize text by just noting the binary presence of words within it\n",
    "        \n",
    "        feats={}\n",
    "\n",
    "        tokens=nltk.word_tokenize(poem.lower())\n",
    "        for token in tokens:\n",
    "            if token not in feature_vocab:\n",
    "                feature_vocab[token]=len(feature_vocab)\n",
    "            feats[feature_vocab[token]]=1\n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for poem in others:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695d6fb",
   "metadata": {},
   "source": [
    "This method yields an NMI of ~0.07 (with some variability due to the randomness of KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b420ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(haiku, others, unigram_featurize_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e57de6",
   "metadata": {},
   "source": [
    "**Q1**: Copy the `unigram_featurize_all` code above and adapt it to create your own featurization method named `fancy_featurize_all`.  You may use whatever information you like to represent these poems for the purposes of clustering them into two categories, but you must use the KMeans clustering (with 2 clusters) as defined in `run_all`.  Use your own understanding of haiku, or read the Long and So article above for other ideas.  Are you able to improve over an NMI of 0.07?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b47e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_featurize_all(haiku, others):\n",
    "    \n",
    "    # your code here \n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06afb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(haiku, others, fancy_featurize_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feda2b4",
   "metadata": {},
   "source": [
    "**Q2**: Describe your method for featurization in 100 words and why you expect it to be able to separate haiku poems from non-haiku poems in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4ffea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
