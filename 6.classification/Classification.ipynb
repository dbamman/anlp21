{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores feature engineering for text classification, training a regularized logistic regression model for the binary classification task of predicting a movie's genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read movie metadata and return dictionary mapping genres to the set of movies that are tagged with that genre; print out the top 30 genres by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata(filename):\n",
    "\n",
    "    metadata={}\n",
    "    counts=Counter()\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        file.readline()\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            movieID=cols[0]\n",
    "            genres=json.loads(cols[8])\n",
    "            \n",
    "            for genre in genres.values():\n",
    "                counts[genre]+=1\n",
    "                if genre not in metadata:\n",
    "                    metadata[genre]={}\n",
    "                metadata[genre][movieID]=1\n",
    "\n",
    "    # print out the 30 most frequent genres in the data\n",
    "    for k, v in counts.most_common(30):\n",
    "        print(\"%s\\t%s\" % (str(v).ljust(10), k))\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=read_metadata(\"../data/movie.metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the metadata to just those movies that exclusively appear with one of two genres selected for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_genres(metadata, genre1, genre2):\n",
    "    \n",
    "    labels={}\n",
    "    \n",
    "    numGen1=0\n",
    "    numGen2=0\n",
    "    \n",
    "    for movie in metadata[genre1]:\n",
    "        if movie not in metadata[genre2]:\n",
    "            labels[movie]=1\n",
    "            numGen1+=1\n",
    "            \n",
    "    for movie in metadata[genre2]:\n",
    "        if movie not in metadata[genre1]:\n",
    "            labels[movie]=0\n",
    "            numGen2+=1\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre1=\"Romantic comedy\"\n",
    "genre2=\"Science Fiction\"\n",
    "\n",
    "labels=filter_for_genres(metadata, genre1, genre2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read movie summary data and tokenize the descriptions of those movies that match for the two genres selected.  Return a list of tokenized summaries and a corresponding list of their binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, labels):\n",
    "    \n",
    "    data=[]\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    \n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            movieID=cols[0]\n",
    "            if movieID in labels:\n",
    "                # lowercase description\n",
    "                description=cols[1].lower()\n",
    "                \n",
    "                # tokenize\n",
    "                tokens=nltk.word_tokenize(description)\n",
    "                X.append(tokens)\n",
    "                Y.append(labels[movieID])  \n",
    "                \n",
    "    return X, Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y=read_data(\"../data/plot_summaries.txt\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets (hold out 20% of the data for evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, devX, trainY, devY = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dataX, feature_functions):\n",
    "    \n",
    "    \"\"\" This function featurizes the data according to the list of parameter feature_functions \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    for tokens in dataX:\n",
    "        feats={}\n",
    "        \n",
    "        for function in feature_functions:\n",
    "            feats.update(function(tokens))\n",
    "\n",
    "        data.append(feats)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_ids(data, feature_vocab):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to a sparse representation\n",
    " that we can fit in a scikit-learn model.  This is important because almost all feature \n",
    " values will be 0 for most documents (note: why?), and we don't want to save them all in \n",
    " memory.\n",
    "\n",
    "    \"\"\"\n",
    "    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "    for idx,doc in enumerate(data):\n",
    "        for f in doc:\n",
    "            if f in feature_vocab:\n",
    "                new_data[idx,feature_vocab[f]]=doc[f]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, top_n=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    This helper function converts a dictionary of feature names to unique numerical ids. \n",
    "    top_n limits the features to only the n most frequent features observed in the training data \n",
    "    (in terms of the number of documents that contains it).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts=Counter()\n",
    "    for doc in data:\n",
    "        for feat in doc:\n",
    "            counts[feat]+=1\n",
    "\n",
    "    feature_vocab={}\n",
    "\n",
    "    for idx, (k, v) in enumerate(counts.most_common(top_n)):\n",
    "        feature_vocab[k]=idx\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(trainX, devX, trainY, devY, feature_functions):\n",
    "\n",
    "    \"\"\" This function evaluates a list of feature functions on the training/dev data arguments \"\"\"\n",
    "    \n",
    "    trainX_feat=build_features(trainX, feature_functions)\n",
    "    devX_feat=build_features(devX, feature_functions)\n",
    "\n",
    "    # just create vocabulary from features in *training* data.\n",
    "    feature_vocab=create_vocab(trainX_feat, top_n=100000)\n",
    "\n",
    "    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n",
    "    devX_ids=features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    clf = linear_model.LogisticRegression(C=100, solver='lbfgs', penalty='l2', max_iter=10000)\n",
    "    clf.fit(trainX_ids, trainY)\n",
    "    print(\"Accuracy: %.3f\" % clf.score(devX_ids, devY))\n",
    "    \n",
    "    return clf, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple dictionary-based feature: this feature value is set to 1 for a document whenever any word present that dictionary appears in that document (and 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_dictionary=set([\"comedy\", \"love\", \"date\"])\n",
    "scifi_dictionary=set([\"science\", \"ship\", \"alien\"])\n",
    "\n",
    "def dictionary_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        if word in comedy_dictionary:\n",
    "            feats[\"word_in_comedy_dictionary\"]=1\n",
    "        if word in scifi_dictionary:\n",
    "            feats[\"word_in_scifi_dictionary\"]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[dictionary_feature]\n",
    "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this accuracy good or bad?  We need to contextualize this performance against some baseline. A simple one to use is a *majority class* classifier: for every document in the test data, let's just predict whatever class appears the most frequently in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class(trainY, devY):\n",
    "    labelCounts=Counter()\n",
    "    for label in trainY:\n",
    "        labelCounts[label]+=1\n",
    "    majority_class=labelCounts.most_common(1)[0][0]\n",
    "    \n",
    "    correct=0.\n",
    "    for label in devY:\n",
    "        if label == majority_class:\n",
    "            correct+=1\n",
    "            \n",
    "    print(\"%s\\t%.3f\" % (majority_class, correct/len(devY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class(trainY, devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_feature(tokens):\n",
    "    feats={}\n",
    "    for word in tokens:\n",
    "        feats[\"UNIGRAM_%s\" % word]=1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[unigram_feature]\n",
    "clf, vocab=pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the top 10 features with the strongest weights for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(clf, vocab, n=10):\n",
    "    weights=clf.coef_[0]\n",
    "    reverse_vocab=[None]*len(weights)\n",
    "    for k in vocab:\n",
    "        reverse_vocab[vocab[k]]=k\n",
    "\n",
    "    for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "        print(\"%.3f\\t%s\" % (weight, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights(clf, vocab, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_awesome_feature(tokens):\n",
    "    feats={}\n",
    "\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[your_awesome_feature]\n",
    "pipeline(trainX, devX, trainY, devY, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
